# -*- coding: utf-8 -*-
"""Fine Tuned Llama Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yVKuh23PVSUTzwhk7gcUYbk8i7FLy3Vn

### The Notebook shows the detail steps to build model from telugu to english translation

Initially turn on the internet connection in the kaggle notebook
"""

import torch
print(torch.cuda.is_available())

!pip install git+https://github.com/huggingface/trl.git
!pip install -U bitsandbytes
!pip install ipywidgets
!pip install python-dotenv
!pip install -U peft
!pip install matplotlib
!pip install seaborn
!pip install wordcloud
!pip install snowflake-connector-python
!pip install nltk matplotlib
!pip install rouge_score

pip install kaggle_secrets

import os
import torch
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from huggingface_hub import login
from datasets import load_dataset
from trl import SFTTrainer
from peft import LoraConfig, get_peft_model
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from wordcloud import WordCloud
from transformers import DataCollatorWithPadding
import snowflake.connector
import re

hf_tkn = ''
login(token=hf_tkn)

mdl_id = "meta-llama/Llama-3.1-8B-Instruct"

bnb_cfg = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

tok = AutoTokenizer.from_pretrained(mdl_id)
mdl = AutoModelForCausalLM.from_pretrained(
    mdl_id,
    quantization_config=bnb_cfg,
    device_map={"": 0}
)


print(mdl)

print("Testing the model with initial examples (before fine-tuning):")

# Define table schema and the test question
table_ddl = """CREATE TABLE sales_data (sale_id INT, product_name VARCHAR(255), amount DECIMAL(10, 2), year INT)""".lower()
ques = "What is the total sales in 2020?".lower()
test_question = f"Generated SQL for Table schema: {table_ddl} | Question: {ques}"

dev = "cuda:0" if torch.cuda.is_available() else "cpu"

# Ensure that the tokenizer has a pad_token defined
if tok.pad_token is None:
    tok.pad_token = tok.eos_token

# Assuming `init_examples` is a list of initial examples to test
init_examples = [
    test_question,  # Your specific test question example
    "Generated SQL for Table schema: CREATE TABLE sales_data (sale_id INT, product_name VARCHAR(255), amount DECIMAL(10, 2), year INT) | Question: What are the products sold in 2020?".lower()
]

# Run inference on the initial test examples
for i, txt in enumerate(init_examples, 1):
    # Convert the input text to lowercase
    txt = txt.lower()

    # Tokenize the input text
    tok_inp = tok(txt, return_tensors="pt", truncation=True, padding=True).to(dev)

    # Generate model output
    gen_out = mdl.generate(**tok_inp, max_new_tokens=50)

    # Decode the generated output
    init_trans_out = tok.decode(gen_out[0], skip_special_tokens=True)

    # Print the results for each example
    print(f"Initial Example {i}:")
    print(f"Input: {txt}")
    print(f"Initial Output: {init_trans_out}\n")

# Initialize Snowflake connection details
snowflake_account = ""
snowflake_user = ""
snowflake_password = ""
snowflake_database = "FINE_TUNE"
snowflake_schema = "TEST"
snowflake_table = "curated_de_ds_tbl"

# Connect to Snowflake
def get_snowflake_connection():
    conn = snowflake.connector.connect(
        account=snowflake_account,
        user=snowflake_user,
        password=snowflake_password,
        database=snowflake_database,
        schema=snowflake_schema
    )
    return conn

# Establish the connection
conn = get_snowflake_connection()

# Define the SQL query to retrieve data
query = """
SELECT SQL_PROMPT, SQL_CONTEXT, SQL_QUERY
FROM CURATED_DE_TTL_VW
where data_label = 'SELECT'
limit 15000
"""

# Load the data into a DataFrame
df = pd.read_sql(query, conn)

# Close the connection
conn.close()

# Prepare the data by combining SQL_PROMPT and SQL_CONTEXT into 'input_text', and SQL_QUERY into 'output_text'
df['input_text'] = "Generate SQL query for Table schema: " + df['SQL_CONTEXT'] + " | Question: " + df['SQL_PROMPT']
df['output_text'] = df['SQL_QUERY']

# Apply the .str.lower() method to convert all text in the 'input_text' and 'output_text' columns to lowercase
df['input_text'] = df['input_text'].str.lower()
df['output_text'] = df['output_text'].str.lower()

df

def select_fields(dat):
    in_text = dat["input_text"].strip()
    out_text = dat["output_text"].strip()

    return {"in": in_text, "out": out_text}

# List of columns to remove
columns_to_remove = ["SQL_PROMPT", "SQL_CONTEXT", "SQL_QUERY"]

# Use drop() to remove columns
df = df.drop(columns=columns_to_remove)

# Check the DataFrame after column removal
df

df

from datasets import Dataset
ds = Dataset.from_pandas(df[['input_text', 'output_text']])

lora_cfg = LoraConfig(
    r=8,
    target_modules=["q_proj", "o_proj", "k_proj", "v_proj", "gate_proj", "up_proj", "down_proj"],
    task_type="CAUSAL_LM",
)

mdl = get_peft_model(mdl, lora_cfg)

ds = ds.train_test_split(test_size=0.2)

# Now `ds` contains 'train' and 'test' splits
train_dataset = ds['train']
eval_dataset = ds['test']

def formatting_func(data):
    formatted_texts = []

    # Iterate over all records in the input and output fields
    for in_text, out_text in zip(data['input_text'], data['output_text']):
        in_text = in_text.strip()
        out_text = out_text.strip().lower()

        # Format the text as per your requirement
        text = f"<|begin_of_text|> {in_text} | sql: {out_text} <|eot_id|>"
        formatted_texts.append(text)

    return formatted_texts

ds

sft_arguments = transformers.TrainingArguments(
    per_device_train_batch_size=12,
    gradient_accumulation_steps=8,
    warmup_steps=20,
    num_train_epochs=3,  # Set the number of epochs (change 3 to your desired number)
    learning_rate=5e-5,
    weight_decay=0.005,
    fp16=True,
    logging_steps=10,
    output_dir="outputs",
    optim="paged_adamw_8bit"
)


# Initialize the trainer with the prepared data
trainer = SFTTrainer(
    model=mdl,
    train_dataset=ds["train"],
    args=sft_arguments,
    peft_config=lora_cfg,
    formatting_func=formatting_func,
)

trainer.train()

device = 'cuda' if torch.cuda.is_available() else 'cpu'
mdl.to(device)

# Define a simple table DDL context
table_ddl = """CREATE TABLE sales_data (sale_id INT, product_name VARCHAR(255), amount DECIMAL(10, 2), year INT)"""

# Define a list of questions to test the model with
questions = [
    "What is the total sales in 2020?",
    "How many sales were made in 2021?",
    "What is the average sale amount for 2022?",
    "Which product had the highest sale amount in 2020?",
    "Generate a query to find the total sales for each product."
]

# Generate a formatted test question for each example
init_examples = [
    f"Generate SQL query for Table schema: {table_ddl} | Question: {ques} | ```SQL: " for ques in questions
]

# Ensure padding token is set for tokenizer
if tok.pad_token is None:
    tok.pad_token = tok.eos_token

tok.pad_token_id = tok.eos_token_id

print(f"Tokenizer loaded with {len(tok)} tokens and pad_token set to {tok.pad_token}")

# Regular expression to match the SQL part between ```sql: and ```
sql_pattern = r"```sql:(.*?)```"

# Tokenize and run inference
for i, txt in enumerate(init_examples, 1):
    # Convert to lowercase for consistency
    txt = txt.lower()

    # Tokenize the input text, ensure padding is applied correctly
    tok_inp = tok(txt, return_tensors="pt", truncation=True, padding=True).to('cuda')

    # Generate output from the model
    gen_out = mdl.generate(**tok_inp, max_new_tokens=50)

    # Decode the generated output
    init_trans_out = tok.decode(gen_out[0], skip_special_tokens=True)

    # Extract the SQL part using regex
    sql_match = re.search(sql_pattern, init_trans_out, re.DOTALL)

    if sql_match:
        sql_query = sql_match.group(1).strip()
        print(f"Test {i}:")
        print(f"Input: {txt}")
        print(f"Generated SQL Query: {sql_query}\n")
    else:
        print(f"Test {i}: No SQL query found in the generated output.\n")

output_dir = "./quantized_model"
mdl.save_pretrained(output_dir)  # Saves the model weights, config, and tokenizer


tok.save_pretrained(output_dir)

print(f"Model saved to {output_dir}")

tok

!pip install evaluate

# Directory where the model was saved
output_dir = "./quantized_model"

# Re-load the model configuration from the saved directory
bnb_cfg = BitsAndBytesConfig(
    load_in_4bit=True,  # Ensure model is loaded in 4-bit precision
    bnb_4bit_quant_type="nf4",  # Specify the same quantization type
    bnb_4bit_compute_dtype=torch.bfloat16  # Ensure computations use bfloat16
)

# Load the tokenizer
tok = AutoTokenizer.from_pretrained(output_dir)


# Update the model's config to match the tokenizer's special tokens
tok.pad_token = tok.eos_token  # Ensure padding uses the eos token
print(f"Tokenizer loaded with {len(tok)} tokens")

# Load the model with quantization settings from the saved directory
mdl = AutoModelForCausalLM.from_pretrained(output_dir, config=bnb_cfg)

# Resize the model's token embeddings to match the new tokenizer size
mdl.resize_token_embeddings(len(tok))

import evaluate
from tqdm import tqdm

# Load the BLEU metric using `evaluate`
bleu_metric = evaluate.load("bleu")


# Prepare test data
test_data = ds['test'][:3000]
input_texts = test_data['input_text']
references = [[ref] for ref in test_data['output_text']]  # BLEU expects a list of reference lists

# Generate predictions with concise formatting
def extract_sql_snippet(text):
    """Extract the SQL code between ```sql and ``` and reformat it."""
    start_marker = "```sql"
    end_marker = "```"
    start_idx = text.find(start_marker)
    end_idx = text.find(end_marker, start_idx + len(start_marker))

    if start_idx != -1 and end_idx != -1:
        sql_content = text[start_idx + len(start_marker):end_idx].strip()
        return f"{start_marker} {sql_content} {end_marker}"
    else:
        return text

predictions = []
for input_text in tqdm(input_texts, desc="Generating predictions"):
    # Tokenize input
    inputs = tok('<|begin_of_text|> ' + input_text + '| ```sql:', return_tensors="pt", truncation=True, padding=True).to('cuda')
    # Generate prediction
    outputs = mdl.generate(**inputs, max_new_tokens=50)
    # Decode prediction
    pred_text = tok.decode(outputs[0], skip_special_tokens=True)
    # Extract and format SQL snippet
    concise_text = extract_sql_snippet(pred_text)
    predictions.append(concise_text)

predictions

def clean_predictions(predictions):
    cleaned_predictions = []
    for pred in predictions:
        # Strip leading/trailing spaces and fix space around colons
        cleaned = pred.strip().replace(" : ", " ")
        cleaned_predictions.append(cleaned)
    return cleaned_predictions

# Cleaned predictions
cleaned = clean_predictions(predictions)

# Display results
for i, (original, cleaned_pred) in enumerate(zip(predictions, cleaned), 1):
    print(f"Original {i}: {original}")
    print(f"Cleaned  {i}: {cleaned_pred}\n")

individual_bleu_scores = []

# Compute BLEU score for each sample
for pred, ref in zip(cleaned, references):
    score = bleu_metric.compute(predictions=[pred], references=[[ref]])
    individual_bleu_scores.append(score['bleu'])

# Compute the average BLEU score
avg_bleu_score = sum(individual_bleu_scores) / len(individual_bleu_scores)
print(f"Average BLEU Score: {avg_bleu_score:.4f}")

# Plot Individual BLEU Scores with Average Line
plt.figure(figsize=(10, 6))
plt.plot(range(len(individual_bleu_scores)), individual_bleu_scores, marker='o', linestyle='-', color='b', label='Individual BLEU Scores')
plt.axhline(y=avg_bleu_score, color='r', linestyle='--', label=f'Average BLEU Score = {avg_bleu_score:.4f}')
plt.title("Individual BLEU Scores for Test Dataset")
plt.xlabel("Test Sample Index")
plt.ylabel("BLEU Score")
plt.legend()
plt.grid()
plt.show()

import evaluate
from tqdm import tqdm

# Load the ROUGE metric using `evaluate`
rouge_metric = evaluate.load("rouge")

rouge_score = rouge_metric.compute(predictions=cleaned, references=references)
print(f"ROUGE Score: {rouge_score}")

# Extract individual ROUGE scores
rouge1_f1 = rouge_score['rouge1']
rouge2_f1 = rouge_score['rouge2']
rougeL_f1 = rouge_score['rougeL']

# If rouge1, rouge2, and rougeL return single numeric values, use them directly:
rouge_f1_scores = [rouge1_f1, rouge2_f1, rougeL_f1]

# Plot the individual ROUGE F1 scores
plt.figure(figsize=(10, 6))
plt.bar(['ROUGE-1', 'ROUGE-2', 'ROUGE-L'], rouge_f1_scores, color='b')
plt.title("ROUGE F1 Scores for Test Dataset")
plt.ylabel("F1 Score")
plt.xlabel("ROUGE Metric")
plt.grid(True)
plt.show()

import evaluate
from tqdm import tqdm

# Load the METEOR metric using `evaluate`
meteor_metric = evaluate.load("meteor")

# Compute METEOR score for each individual prediction
individual_meteor_scores = []
for pred, ref in zip(cleaned, references):
    score = meteor_metric.compute(predictions=[pred], references=[ref])
    individual_meteor_scores.append(score['meteor'])

# Calculate the average METEOR score
average_meteor_score = sum(individual_meteor_scores) / len(individual_meteor_scores)

# Plot the individual METEOR scores
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(individual_meteor_scores) + 1), individual_meteor_scores, marker='o', linestyle='-', color='r', label="Individual METEOR Score")
plt.axhline(y=average_meteor_score, color='b', linestyle='--', label=f"Average METEOR Score: {average_meteor_score:.4f}")
plt.title("Individual METEOR Scores for Test Dataset with Average")
plt.xlabel("Test Sample Index")
plt.ylabel("METEOR Score")
plt.grid(True)
plt.xticks(range(1, len(individual_meteor_scores) + 1))  # Ensure x-axis shows 1 to N indices
plt.yticks([i/10 for i in range(11)])  # Set y-ticks from 0.0 to 1.0 for METEOR scores
plt.legend()
plt.show()

# Print the average METEOR score
print(f"Average METEOR Score: {average_meteor_score:.4f}")

