{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a3a5521-89a6-40e1-8ec0-9f8d72c17ce2",
      "metadata": {
        "id": "7a3a5521-89a6-40e1-8ec0-9f8d72c17ce2",
        "outputId": "0cfb8075-4cc0-41ef-f8c4-0c1347a6730c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.44.1)\n",
            "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\n",
            "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.2)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\n",
            "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2023.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cd5d200-f0be-4f29-b287-9bb809b67ea6",
      "metadata": {
        "id": "3cd5d200-f0be-4f29-b287-9bb809b67ea6",
        "outputId": "9cb7dcbb-bd91-47dc-bdb0-74be7d3ab693",
        "colab": {
          "referenced_widgets": [
            "c2c5fc6eab794ec382e812058e829419"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer loaded with 128256 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2c5fc6eab794ec382e812058e829419",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Embedding(128256, 4096)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Directory where the model was saved\n",
        "output_dir = \"./quantized_model\"\n",
        "\n",
        "# Re-load the model configuration from the saved directory\n",
        "bnb_cfg = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # Ensure model is loaded in 4-bit precision\n",
        "    bnb_4bit_quant_type=\"nf4\",  # Specify the same quantization type\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16  # Ensure computations use bfloat16\n",
        ")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "\n",
        "# Update the model's config to match the tokenizer's special tokens\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "print(f\"Tokenizer loaded with {len(tokenizer)} tokens\")\n",
        "\n",
        "# Load the model with quantization settings from the saved directory\n",
        "model = AutoModelForCausalLM.from_pretrained(output_dir, config=bnb_cfg)\n",
        "\n",
        "# Resize the model's token embeddings to match the new tokenizer size\n",
        "model.resize_token_embeddings(len(tokenizer))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b320ed2b-c472-4450-909b-716aca36bf3a",
      "metadata": {
        "id": "b320ed2b-c472-4450-909b-716aca36bf3a",
        "outputId": "c6620dad-ad9f-42c1-f05b-58c5fff404cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test 1:\n",
            "Input: <|begin_of_text|> generate sql query for table schema: create table sales_data (sale_id int, product_name varchar(255), amount decimal(10, 2), year int) | question: what is the total sales in 2020? | sql: \n",
            "Generated SQL Query:  generate sql query for table schema: create table sales_data (sale_id int, product_name varchar(255), amount decimal(10, 2), year int) | question: what is the total sales in 2020? | sql:  select sum(amount) from sales_data where year = 2020; | sql:  select sum(amount) from sales_data where year = 2020;   ```sql select sum(amount) from sales_data where year = 2020;```\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test 2:\n",
            "Input: <|begin_of_text|> generate sql query for table schema: create table sales_data (sale_id int, product_name varchar(255), amount decimal(10, 2), year int) | question: how many sales were made in 2021? | sql: \n",
            "Generated SQL Query:  generate sql query for table schema: create table sales_data (sale_id int, product_name varchar(255), amount decimal(10, 2), year int) | question: how many sales were made in 2021? | sql:  ```sql select count(*) from sales_data where year = 2021;```  : sql: create table sales_data (sale_id int, product_name varchar(255), amount decimal(10, 2), year int); select count(*) from\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test 3:\n",
            "Input: <|begin_of_text|> generate sql query for table schema: create table sales_data (sale_id int, product_name varchar(255), amount decimal(10, 2), year int) | question: what is the average sale amount for 2022? | sql: \n",
            "Generated SQL Query:  generate sql query for table schema: create table sales_data (sale_id int, product_name varchar(255), amount decimal(10, 2), year int) | question: what is the average sale amount for 2022? | sql:  select avg(amount) from sales_data where year = 2022; | sql: 1 | sql: |  select avg(amount) from sales_data where year = 2022; | question: what is the total sale amount for 202\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test 4:\n",
            "Input: <|begin_of_text|> generate sql query for table schema: create table sales_data (sale_id int, product_name varchar(255), amount decimal(10, 2), year int) | question: which product had the highest sale amount in 2020? | sql: \n",
            "Generated SQL Query:  generate sql query for table schema: create table sales_data (sale_id int, product_name varchar(255), amount decimal(10, 2), year int) | question: which product had the highest sale amount in 2020? | sql:  select product_name, amount from sales_data where year = 2020 order by amount desc limit 1; | sql: ```sql select product_name, amount from sales_data where year = 2020 order by amount desc limit 1;```\n",
            "\n",
            "Test 5:\n",
            "Input: <|begin_of_text|> generate sql query for table schema: create table sales_data (sale_id int, product_name varchar(255), amount decimal(10, 2), year int) | question: generate a query to find the total sales for each product. | sql: \n",
            "Generated SQL Query:  generate sql query for table schema: create table sales_data (sale_id int, product_name varchar(255), amount decimal(10, 2), year int) | question: generate a query to find the total sales for each product. | sql:  ```sql select product_name, sum(amount) as total_sales from sales_data group by product_name;```  : | sql: ```sql select product_name, sum(amount) as total_sales from sales_data where year = 2022 group by product\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Proceed with your inference setup\n",
        "table_ddl = \"\"\"CREATE TABLE sales_data (sale_id INT, product_name VARCHAR(255), amount DECIMAL(10, 2), year INT)\"\"\"\n",
        "\n",
        "# Define a list of questions to test the model with\n",
        "questions = [\n",
        "    \"What is the total sales in 2020?\",\n",
        "    \"How many sales were made in 2021?\",\n",
        "    \"What is the average sale amount for 2022?\",\n",
        "    \"Which product had the highest sale amount in 2020?\",\n",
        "    \"Generate a query to find the total sales for each product.\"\n",
        "]\n",
        "\n",
        "# Generate a formatted test question for each example\n",
        "init_examples = [\n",
        "    f\"<|begin_of_text|> Generate SQL query for Table schema: {table_ddl} | Question: {ques} | SQL: \" for ques in questions\n",
        "]\n",
        "\n",
        "# Ensure model is on the correct device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Perform inference\n",
        "for i, txt in enumerate(init_examples, 1):\n",
        "    # Convert to lowercase for consistency\n",
        "    txt = txt.lower()\n",
        "\n",
        "    # Tokenize the input text\n",
        "    tok_inp = tokenizer(txt, return_tensors=\"pt\", truncation=True, padding=True).to(device)  # Match device\n",
        "\n",
        "    # Generate output from the model\n",
        "    gen_out = model.generate(**tok_inp, max_new_tokens=50)\n",
        "\n",
        "    # Decode the generated output (i.e., the SQL query)\n",
        "    init_trans_out = tokenizer.decode(gen_out[0], skip_special_tokens=True)\n",
        "\n",
        "    # Print the results for each example\n",
        "    print(f\"Test {i}:\")\n",
        "    print(f\"Input: {txt}\")\n",
        "    print(f\"Generated SQL Query: {init_trans_out}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "630e5877-2792-4ed6-ae16-f0759eba6433",
      "metadata": {
        "id": "630e5877-2792-4ed6-ae16-f0759eba6433"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}